{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2933f2",
   "metadata": {},
   "source": [
    "# Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf750d0",
   "metadata": {},
   "source": [
    "## Logical Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ed7fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194d7ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Learning-Curve\\Langchain\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1643: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from pydantic import Field, BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route the user to the most relevant datasource.\"\"\"\n",
    "    \n",
    "    datasource : Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(..., description=\"Given the user question choose which of the datasource will be the most relevant.\")\n",
    "    \n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# LLM with structured output parser\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"\n",
    "You are an expert in routing user's question to appropriate datasource.\n",
    "Based on the programming language the question is referring to, route it to relevant datasource.  \n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "   [\n",
    "       (\"system\", system),\n",
    "       (\"human\", \"{question}\")\n",
    "   ]\n",
    ")\n",
    "\n",
    "# Router Chain\n",
    "router = prompt | structured_llm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871d1b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "question= \"\"\"\n",
    "Why this code is not working?\n",
    "\n",
    "from langchina_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "\n",
    "template = '''You are an helpful assistant answer the question: '''\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    template\n",
    ")\n",
    "\n",
    "chain =  prompt | llm | parser\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dca7ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up route logic\n",
    "\n",
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in result.datasource.lower():\n",
    "        return \"chain on js_docs\"\n",
    "    elif \"golang_docs\" in result.datasource.lower():\n",
    "        return \"chain on golang_docs\"\n",
    "    \n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "final_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "782c66c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = final_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df2a3769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain for python_docs\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b134c60b",
   "metadata": {},
   "source": [
    "## Semantic Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae24a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "A black hole is a region in space where gravity is so strong that nothing, not even light, can escape from it. This phenomenon occurs when a massive star collapses in upon itself, creating a dense core with a gravitational pull strong enough to trap everything within its event horizon. This means that once an object crosses the event horizon of a black hole, it is impossible for it to escape, making black holes one of the most mysterious and fascinating objects in the universe.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Two prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor \\n\n",
    "You are great at answering the physics questions in clear and concise manner. \\n\n",
    "When you don't know the answer to the question, you admit it.\n",
    "\n",
    "Here is the question: {question} \n",
    "\"\"\"\n",
    "\n",
    "maths_template = \"\"\"You are a very smart mathematics professor \\n\n",
    "You are great at answering the mathematics questions in clear and concise manner. \\n\n",
    "When you don't know the answer to the question, you admit it.\n",
    "\n",
    "Here is the question: {question} \n",
    "\"\"\"\n",
    "\n",
    "# Embedded Prompts\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "prompt_templates = [physics_template, maths_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "# Route question to the prompt\n",
    "def prompt_router(input):\n",
    "    # Embed input\n",
    "    query_embedding = embeddings.embed_query(input['question'])\n",
    "    # Computing similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "\n",
    "    # Chosen prompt\n",
    "    print(\"Using MATH\" if most_similar == maths_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "chain = (\n",
    "    {\"question\" : RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What is blackhole?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6496889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MATH\n",
      "Inferential statistics is the branch of statistics that involves making inferences or conclusions about a population based on data collected from a sample of that population. It allows us to generalize our findings from the sample to the larger population. This is done through hypothesis testing, confidence intervals, and other methods to determine the likelihood of an event occurring or the relationship between variables in the population.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"What is Inferential statistics?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
