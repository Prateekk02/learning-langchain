{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9448097b",
   "metadata": {},
   "source": [
    "# Query Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a78f0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "path = \".env\"\n",
    "load_dotenv(dotenv_path=path)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3430ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Document \n",
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=('post-content', 'post-title','post-header')\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "doc = loader.load()\n",
    "\n",
    "# Split \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "split = text_splitter.split_documents(doc)\n",
    "\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=split, embedding= OpenAIEmbeddings())\n",
    " \n",
    "# Retriever\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5bb41",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6846dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "template=\"\"\"You are an AI language model assistant. Your task is to generate exactly five \n",
    "different versions of the given user question to retrieve relevant documents from a\n",
    "vector database. By generating multiple perspectives on the user question, your goal is to help\n",
    "overcome limitations of distance-based similarity search.\n",
    "\n",
    "Follow these output rules:\n",
    "- Generate EXACTLY five alternative questions\n",
    "- Use ONLY newlines as separators (no numbers, bullets, or extra text)\n",
    "- Do not include any additional explanations\n",
    "\n",
    "Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspective = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries= (\n",
    "    prompt_perspective\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | RunnableLambda(lambda x: x.split(\"\\n\"))\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39611b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "from typing import List\n",
    "\n",
    "def get_unique_union(documents: List[list]):\n",
    "    \"\"\"Unique union of retrieved documents.\"\"\"\n",
    "    # Flatten list of list and convert it into string\n",
    "    flatten_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    \n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flatten_docs))\n",
    "    \n",
    "    # Return \n",
    "    return [loads(doc) for doc in unique_docs] \n",
    "\n",
    "# Retrieve\n",
    "question = \"What is the decomposition for LLM agents?\"\n",
    "retrieval_chain= generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e2e57",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff8cbfd",
   "metadata": {},
   "source": [
    "### Multi-Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8802202",
   "metadata": {},
   "source": [
    "![multi query RAG](../static/images/Multi-Query.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05be964a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The decomposition for LLM agents includes breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from operator import itemgetter \n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on the context.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = {\"context\": retrieval_chain, \"question\" : itemgetter(\"question\")} | prompt | llm | StrOutputParser() \n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70fa39",
   "metadata": {},
   "source": [
    "### RAG Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d638c",
   "metadata": {},
   "source": [
    "![RAG-Fusion](../static/images/RAG-Fusion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdc21f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion \n",
    "template=\"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search query related to: {question} \\n\n",
    "Output (4 queries):\n",
    "\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba1274dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries=(\n",
    "    prompt_rag_fusion\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | RunnableLambda(lambda x : x.split('\\n'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ad2818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciporcal_rank_fusion(results: list[list], k = 60):\n",
    "    '''Reciporcal rank fusion that takes multiple lists of ranked documents and an optional\n",
    "        parameter k used in the RRF formula\n",
    "    '''\n",
    "    \n",
    "    # Initialized a dictionary to hold fused scores for each unique documents\n",
    "    fused_score = {}\n",
    "    \n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results: \n",
    "        # Iterate through each document in the list, with its rank (position in the list) \n",
    "        \n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document into string format to use as a key \n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not in fused_score dictionary, add it to fused_score list with rank 0\n",
    "            \n",
    "            if doc_str not in fused_score:\n",
    "                fused_score[doc_str] = 0\n",
    "            \n",
    "            # Retrieved the current score of the document\n",
    "            previous_score = fused_score[doc_str]\n",
    "            \n",
    "            # Update the score of the document using the RFF formula : 1 / (rank + k)\n",
    "            fused_score[doc_str] += 1 / (rank + k) \n",
    "        \n",
    "    # Sort the documents based on their fused score in descending order to get the final reranked results\n",
    "    reranked_results = [(loads(doc),score) for doc, score in sorted(fused_score.items(), key= lambda x:x[1], reverse= True ) ]\n",
    "    \n",
    "    # Return the reranked results based on their fused scores in descending order to get the final reranked results\n",
    "    return reranked_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1becf6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciporcal_rank_fusion  \n",
    "\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44b1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The decomposition for LLM agents includes breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. Additionally, the agent can do self-criticism and self-reflection over past actions, learn from mistakes, and refine them for future steps to improve the quality of final results.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# RAG\n",
    "\n",
    "template=\"\"\"Answer the following question based on this context: {context}\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = {\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter(\"question\")} | prompt | llm | StrOutputParser()\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad49895",
   "metadata": {},
   "source": [
    "## Decomposition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "542b0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition \n",
    "template=\"\"\"You are a helpful assistant that generate multiple sub-questions related to input question.\\n\n",
    "The goal is to break down the input in a set of sub-problems / sub-questions that can be answered in isolation. \\n\n",
    "Generate multiple search queries related to question : {question} \\n\n",
    "Output (3 queries):       \n",
    "\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a276b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain \n",
    "generated_queries_decomposition = prompt_decomposition | llm | StrOutputParser() | RunnableLambda(lambda x: x.split(\"\\n\") )\n",
    "\n",
    "# Run \n",
    "question  =  \"What are the main components of an LLM-powered autonomous agent system\" \n",
    "\n",
    "questions = generated_queries_decomposition.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9573966f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is LLM technology and how is it used in autonomous agent systems?',\n",
       " '2. What are the key components of an autonomous agent system?',\n",
       " '3. How does an LLM-powered autonomous agent system differ from other types of autonomous systems?']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc60de",
   "metadata": {},
   "source": [
    "![Decomposition](../static/images/decomposition.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9575e28",
   "metadata": {},
   "source": [
    "Papers:\n",
    "- https://arxiv.org/pdf/2205.10625.pdf\n",
    "- https://arxiv.org/pdf/2212.10509.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04310279",
   "metadata": {},
   "source": [
    "#### Answer recursively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5dda6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "    \\n --- \\n {question} \\n --- \\n\n",
    "    \n",
    "    Here is any any available background question + answer pairs:\n",
    "    \n",
    "    \\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "    \n",
    "    Here is additional context relevant to question:\n",
    "    \n",
    "    \\n --- \\n {context} \\n --- \\n     \n",
    "    Use the above context and any background questions + answer pairs to answer the question: \\n {question}    \n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d202f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q & A Pairs\"\"\"\n",
    "    formatted_string=\"\" \n",
    "    formatted_string += f\"Question: {question} \\n Answer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "# Recursively call for each question.\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\") , \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()        \n",
    "    )  \n",
    "    \n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pairs = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n --- \\n\" + q_a_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd057fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An LLM-powered autonomous agent system differs from other types of autonomous systems in that it utilizes a large language model (LLM) as its core controller, serving as the agent's brain. This system is complemented by key components such as planning, which involves breaking down tasks into subgoals and reflecting on past actions for refinement. Additionally, the system relies on natural language interfaces for communication with external components like memory and tools. The use of LLMs in autonomous agents allows for powerful problem-solving capabilities beyond just generating text, stories, or programs.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd7b43",
   "metadata": {},
   "source": [
    "#### Answering each sub-question individially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a110664b",
   "metadata": {},
   "source": [
    "![decomposition indivisually](../static/images/decomposition_indivisually.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f8a3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# RAG Prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-questions\"\"\"\n",
    "    \n",
    "    # Use sub_question_generator_chain to generate subquestions\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\": question}) \n",
    "    \n",
    "    # Initialize list to hold RAG Chain\n",
    "    rag_result = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve relevant documents from retriever\n",
    "        retrieved_docs= retriever.get_relevant_documents(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and \n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \"question\": sub_question})\n",
    "        \n",
    "        rag_result.append(answer)\n",
    "        \n",
    "    return rag_result, sub_questions\n",
    "\n",
    "# Wrap the retrieval & RAG process in a RunnableLambda for integrating it to chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generated_queries_decomposition) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a948bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: 1. What is LLM technology and how does it work in autonomous agent systems?\n",
      "Answer 1: LLM technology refers to using large language models as the core controller in autonomous agent systems. In these systems, LLM functions as the agent's brain, enabling tasks such as planning, subgoal decomposition, reflection, refinement, and memory utilization. The reliability of natural language interfaces between LLMs and external components is a current challenge due to potential formatting errors and rebellious behavior.\n",
      "\n",
      "Question 2: 2. What are the key components of an autonomous agent system?\n",
      "Answer 2: The key components of an autonomous agent system include planning with subgoal decomposition, reflection and refinement, and memory. These components enable the agent to break down tasks, learn from past actions, and improve future results. The system is powered by a large language model acting as the agent's brain.\n",
      "\n",
      "Question 3: 3. How does LLM technology enhance the performance of autonomous agents in various applications?\n",
      "Answer 3: LLM technology enhances the performance of autonomous agents by serving as the agent's brain and enabling efficient handling of complex tasks through subgoal decomposition and reflection. It allows agents to learn from mistakes, refine actions, and improve the quality of final results. LLM-powered agents can handle autonomous design, planning, and performance of complex scientific experiments by leveraging tools, browsing the Internet, and executing code.\n"
     ]
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q & A pairs\"\"\"\n",
    "    formatted_string = \"\" \n",
    "    \n",
    "    for i , (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "        \n",
    "    return formatted_string.strip()\n",
    "    \n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "545e9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template=\"\"\"Here is set of Q&A pairs: \n",
    "\n",
    "{context}\n",
    "Use these to synthesize an answer to the question: {question} \n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain2 = prompt | llm | StrOutputParser()\n",
    "\n",
    "result = final_rag_chain2.invoke({\"question\": question, \"context\": context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69bc77c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main components of an LLM-powered autonomous agent system include the large language model (LLM) acting as the agent's brain, planning with subgoal decomposition, reflection and refinement, and memory utilization. These components work together to enable the agent to efficiently handle complex tasks, learn from past actions, and improve future performance. The LLM technology enhances the agent's ability to autonomously design, plan, and execute tasks by leveraging natural language interfaces, browsing the Internet, and executing code.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef9ae8",
   "metadata": {},
   "source": [
    "## Step-Back Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6f47e063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert at world knowledge. Your task is to step back and paraphrase\\n            a question to a more generic step-back question. '), additional_kwargs={}), FewShotChatMessagePromptTemplate(examples=[{'input': 'Could the member of The Police perform lawful arrest?', 'output': 'What can the member of Police do?'}, {'input': 'Anakin Skywalker was born on which planet?', 'output': \"What is Anakin Skywalker's personal history?\"}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Few Shot example\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "example = [\n",
    "    {\n",
    "        \"input\": \"Could the member of The Police perform lawful arrest?\",\n",
    "        \"output\": \"What can the member of Police do?\"\n",
    "    }, \n",
    "    {\n",
    "        \"input\": \"Anakin Skywalker was born on which planet?\",\n",
    "        \"output\": \"What is Anakin Skywalker's personal history?\"\n",
    "    },\n",
    "]   \n",
    "\n",
    "# Transforming these to example messages\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\")\n",
    "    ]\n",
    ") \n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=example\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase\n",
    "            a question to a more generic step-back question. \"\"\"\n",
    "        ),\n",
    "        \n",
    "        # Few Shot prompt\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\")   ,     \n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e32d2cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the process of breaking down tasks into smaller components?'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_stepback = prompt | llm | StrOutputParser() \n",
    "question = \"What is task decomposition?\"\n",
    "generate_queries_stepback.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "600f07d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a crucial concept in the field of artificial intelligence and problem-solving. It involves breaking down a complex task into smaller, more manageable subtasks or steps. This process allows an agent or a system to better understand the task at hand and plan ahead effectively.\\n\\nOne common technique for task decomposition is the Chain of Thought (CoT), which prompts the model to think step by step, enabling it to decompose difficult tasks into simpler components. This approach helps in utilizing more computational resources during testing and sheds light on the model\\'s thinking process.\\n\\nAnother extension of CoT is the Tree of Thoughts, which explores multiple reasoning possibilities at each step of the task decomposition. By generating multiple thoughts per step and creating a tree structure, this method enhances the agent\\'s ability to tackle complex tasks.\\n\\nTask decomposition can be achieved through various methods, such as using simple prompts like \"Steps for XYZ\" or task-specific instructions tailored to the nature of the task. Additionally, the LLM+P approach involves relying on an external classical planner for long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) to describe the planning problem and outsourcing the planning step to an external tool.\\n\\nIn summary, task decomposition is a fundamental strategy for breaking down complex tasks into more manageable components, enabling agents and systems to plan and execute tasks more effectively and efficiently.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response prompt \n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. \n",
    "I am going to ask you a question. Your response should be comprehensive and based on the following context\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original question : {question}\n",
    "# Answer: \n",
    "\"\"\"\n",
    "\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question.\n",
    "        \"normal_context\": RunnableLambda(lambda x : x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_stepback | retriever, \n",
    "        # Pass on the question\n",
    "        \"question\": RunnableLambda(lambda x : x['question']) \n",
    "    }\n",
    "    | response_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c5879",
   "metadata": {},
   "source": [
    "## HyDE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15eac5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a crucial concept in the field of reinforcement learning for Large Language Models (LLMs). LLM agents are complex models that are trained to perform a wide range of tasks, such as natural language understanding, generation, and translation. Task decomposition involves breaking down a complex task into smaller, more manageable sub-tasks that can be solved independently or in sequence.\\n\\nBy decomposing a task, LLM agents can effectively leverage their capabilities to solve complex problems more efficiently. This approach allows the agent to focus on solving smaller, more specific sub-tasks, which can lead to improved performance and faster learning. Additionally, task decomposition can help LLM agents generalize better to new tasks by learning reusable sub-tasks that can be applied across different domains.\\n\\nOverall, task decomposition is a powerful technique for enhancing the capabilities of LLM agents and enabling them to tackle a wide range of complex tasks in a more efficient and effective manner.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HyDE document generation\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question. \\n\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run \n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15fcf4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.')]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever\n",
    "\n",
    "retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "\n",
    "print(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e366d473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents can be done by using simple prompting like \"Steps for XYZ\", \"What are the subgoals for achieving XYZ?\", task-specific instructions, or with human inputs. Additionally, another approach involves relying on an external classical planner to do long-horizon planning, utilizing the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "template = \"\"\" Answer the following question based on the context: {context}\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain_hyde = (\n",
    "    prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain_hyde.invoke({\"question\": question, \"context\": retrieved_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c7bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
