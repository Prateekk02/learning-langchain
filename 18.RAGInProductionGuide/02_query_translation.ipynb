{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9448097b",
   "metadata": {},
   "source": [
    "# Query Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a78f0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "path = \".env\"\n",
    "load_dotenv(dotenv_path=path)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3430ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Document \n",
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=('post-content', 'post-title','post-header')\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "doc = loader.load()\n",
    "\n",
    "# Split \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "split = text_splitter.split_documents(doc)\n",
    "\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=split, embedding= OpenAIEmbeddings())\n",
    " \n",
    "# Retriever\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5bb41",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6846dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "template=\"\"\"You are an AI language model assistant. Your task is to generate exactly five \n",
    "different versions of the given user question to retrieve relevant documents from a\n",
    "vector database. By generating multiple perspectives on the user question, your goal is to help\n",
    "overcome limitations of distance-based similarity search.\n",
    "\n",
    "Follow these output rules:\n",
    "- Generate EXACTLY five alternative questions\n",
    "- Use ONLY newlines as separators (no numbers, bullets, or extra text)\n",
    "- Do not include any additional explanations\n",
    "\n",
    "Original question: {question}\"\"\"\n",
    "\n",
    "prompt_perspective = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries= (\n",
    "    prompt_perspective\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | RunnableLambda(lambda x: x.split(\"\\n\"))\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39611b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "from typing import List\n",
    "\n",
    "def get_unique_union(documents: List[list]):\n",
    "    \"\"\"Unique union of retrieved documents.\"\"\"\n",
    "    # Flatten list of list and convert it into string\n",
    "    flatten_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    \n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flatten_docs))\n",
    "    \n",
    "    # Return \n",
    "    return [loads(doc) for doc in unique_docs] \n",
    "\n",
    "# Retrieve\n",
    "question = \"What is the decomposition for LLM agents?\"\n",
    "retrieval_chain= generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e2e57",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff8cbfd",
   "metadata": {},
   "source": [
    "### Multi-Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8802202",
   "metadata": {},
   "source": [
    "![multi query RAG](../static/images/Multi-Query.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05be964a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The decomposition for LLM agents includes breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from operator import itemgetter \n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on the context.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = {\"context\": retrieval_chain, \"question\" : itemgetter(\"question\")} | prompt | llm | StrOutputParser() \n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70fa39",
   "metadata": {},
   "source": [
    "### RAG Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d638c",
   "metadata": {},
   "source": [
    "![RAG-Fusion](../static/images/RAG-Fusion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdc21f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion \n",
    "template=\"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search query related to: {question} \\n\n",
    "Output (4 queries):\n",
    "\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba1274dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries=(\n",
    "    prompt_rag_fusion\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | RunnableLambda(lambda x : x.split('\\n'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ad2818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciporcal_rank_fusion(results: list[list], k = 60):\n",
    "    '''Reciporcal rank fusion that takes multiple lists of ranked documents and an optional\n",
    "        parameter k used in the RRF formula\n",
    "    '''\n",
    "    \n",
    "    # Initialized a dictionary to hold fused scores for each unique documents\n",
    "    fused_score = {}\n",
    "    \n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results: \n",
    "        # Iterate through each document in the list, with its rank (position in the list) \n",
    "        \n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document into string format to use as a key \n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not in fused_score dictionary, add it to fused_score list with rank 0\n",
    "            \n",
    "            if doc_str not in fused_score:\n",
    "                fused_score[doc_str] = 0\n",
    "            \n",
    "            # Retrieved the current score of the document\n",
    "            previous_score = fused_score[doc_str]\n",
    "            \n",
    "            # Update the score of the document using the RFF formula : 1 / (rank + k)\n",
    "            fused_score[doc_str] += 1 / (rank + k) \n",
    "        \n",
    "    # Sort the documents based on their fused score in descending order to get the final reranked results\n",
    "    reranked_results = [(loads(doc),score) for doc, score in sorted(fused_score.items(), key= lambda x:x[1], reverse= True ) ]\n",
    "    \n",
    "    # Return the reranked results based on their fused scores in descending order to get the final reranked results\n",
    "    return reranked_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1becf6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciporcal_rank_fusion  \n",
    "\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44b1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The decomposition for LLM agents includes breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. Additionally, the agent can do self-criticism and self-reflection over past actions, learn from mistakes, and refine them for future steps to improve the quality of final results.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# RAG\n",
    "\n",
    "template=\"\"\"Answer the following question based on this context: {context}\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = {\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter(\"question\")} | prompt | llm | StrOutputParser()\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad49895",
   "metadata": {},
   "source": [
    "## Decomposition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "542b0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition \n",
    "template=\"\"\"You are a helpful assistant that generate multiple sub-questions related to input question.\\n\n",
    "The goal is to break down the input in a set of sub-problems / sub-questions that can be answered in isolation. \\n\n",
    "Generate multiple search queries related to question : {question} \\n\n",
    "Output (3 queries):       \n",
    "\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a276b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain \n",
    "generated_queries_decomposition = prompt_decomposition | llm | StrOutputParser() | RunnableLambda(lambda x: x.split(\"\\n\") )\n",
    "\n",
    "# Run \n",
    "question  =  \"What are the main components of an LLM-powered autonomous agent system\" \n",
    "\n",
    "questions = generated_queries_decomposition.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9573966f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is LLM technology and how is it used in autonomous agent systems?',\n",
       " '2. What are the key components of an autonomous agent system?',\n",
       " '3. How does an LLM-powered autonomous agent system differ from other types of autonomous systems?']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc60de",
   "metadata": {},
   "source": [
    "![Decomposition](../static/images/decomposition.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9575e28",
   "metadata": {},
   "source": [
    "Papers:\n",
    "- https://arxiv.org/pdf/2205.10625.pdf\n",
    "- https://arxiv.org/pdf/2212.10509.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04310279",
   "metadata": {},
   "source": [
    "#### Answer recursively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5dda6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "    \\n --- \\n {question} \\n --- \\n\n",
    "    \n",
    "    Here is any any available background question + answer pairs:\n",
    "    \n",
    "    \\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "    \n",
    "    Here is additional context relevant to question:\n",
    "    \n",
    "    \\n --- \\n {context} \\n --- \\n     \n",
    "    Use the above context and any background questions + answer pairs to answer the question: \\n {question}    \n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d202f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q & A Pairs\"\"\"\n",
    "    formatted_string=\"\" \n",
    "    formatted_string += f\"Question: {question} \\n Answer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "# Recursively call for each question.\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\") , \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()        \n",
    "    )  \n",
    "    \n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pairs = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n --- \\n\" + q_a_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd057fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An LLM-powered autonomous agent system differs from other types of autonomous systems in that it utilizes a large language model (LLM) as its core controller, serving as the agent's brain. This system is complemented by key components such as planning, which involves breaking down tasks into subgoals and reflecting on past actions for refinement. Additionally, the system relies on natural language interfaces for communication with external components like memory and tools. The use of LLMs in autonomous agents allows for powerful problem-solving capabilities beyond just generating text, stories, or programs.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd7b43",
   "metadata": {},
   "source": [
    "#### Answering each sub-question individially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a110664b",
   "metadata": {},
   "source": [
    "![decomposition indivisually](../static/images/decomposition_indivisually.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f8a3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# RAG Prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-questions\"\"\"\n",
    "    \n",
    "    # Use sub_question_generator_chain to generate subquestions\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\": question}) \n",
    "    \n",
    "    # Initialize list to hold RAG Chain\n",
    "    rag_result = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve relevant documents from retriever\n",
    "        retrieved_docs= retriever.get_relevant_documents(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and \n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \"question\": sub_question})\n",
    "        \n",
    "        rag_result.append(answer)\n",
    "        \n",
    "    return rag_result, sub_questions\n",
    "\n",
    "# Wrap the retrieval & RAG process in a RunnableLambda for integrating it to chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generated_queries_decomposition) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a948bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: 1. What is LLM technology and how does it work in autonomous agent systems?\n",
      "Answer 1: LLM technology refers to using large language models as the core controller in autonomous agent systems. In these systems, LLM functions as the agent's brain, enabling tasks such as planning, subgoal decomposition, reflection, refinement, and memory utilization. The reliability of natural language interfaces between LLMs and external components is a current challenge due to potential formatting errors and rebellious behavior.\n",
      "\n",
      "Question 2: 2. What are the key components of an autonomous agent system?\n",
      "Answer 2: The key components of an autonomous agent system include planning with subgoal decomposition, reflection and refinement, and memory. These components enable the agent to break down tasks, learn from past actions, and improve future results. The system is powered by a large language model acting as the agent's brain.\n",
      "\n",
      "Question 3: 3. How does LLM technology enhance the performance of autonomous agents in various applications?\n",
      "Answer 3: LLM technology enhances the performance of autonomous agents by serving as the agent's brain and enabling efficient handling of complex tasks through subgoal decomposition and reflection. It allows agents to learn from mistakes, refine actions, and improve the quality of final results. LLM-powered agents can handle autonomous design, planning, and performance of complex scientific experiments by leveraging tools, browsing the Internet, and executing code.\n"
     ]
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q & A pairs\"\"\"\n",
    "    formatted_string = \"\" \n",
    "    \n",
    "    for i , (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "        \n",
    "    return formatted_string.strip()\n",
    "    \n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "545e9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template=\"\"\"Here is set of Q&A pairs: \n",
    "\n",
    "{context}\n",
    "Use these to synthesize an answer to the question: {question} \n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain2 = prompt | llm | StrOutputParser()\n",
    "\n",
    "result = final_rag_chain2.invoke({\"question\": question, \"context\": context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69bc77c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main components of an LLM-powered autonomous agent system include the large language model (LLM) acting as the agent's brain, planning with subgoal decomposition, reflection and refinement, and memory utilization. These components work together to enable the agent to efficiently handle complex tasks, learn from past actions, and improve future performance. The LLM technology enhances the agent's ability to autonomously design, plan, and execute tasks by leveraging natural language interfaces, browsing the Internet, and executing code.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef9ae8",
   "metadata": {},
   "source": [
    "## Step-Back Prompting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
