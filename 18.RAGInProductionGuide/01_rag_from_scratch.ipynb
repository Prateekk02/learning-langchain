{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78d9e78",
   "metadata": {},
   "source": [
    "# RAG From Scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0851f3",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1f2216",
   "metadata": {},
   "source": [
    "(1) Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9790f42",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8890/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'http://127.0.0.1:8890/'. Verify the server is running and reachable. (request to http://127.0.0.1:8890/api/kernels?1750858210531 failed, reason: connect ECONNREFUSED 127.0.0.1:8890).)."
     ]
    }
   ],
   "source": [
    "!pip install -q langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e189e4",
   "metadata": {},
   "source": [
    "(2) LangSmith "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f3f4a",
   "metadata": {},
   "source": [
    "### Step 1: Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e1c9d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "path = \".env\"\n",
    "load_dotenv(dotenv_path=path)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef98fae",
   "metadata": {},
   "source": [
    "#### INDEXING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab037edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prateek Kumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks to enhance model performance. Different methods like Chain of Thought and Tree of Thoughts are used to decompose tasks and improve understanding of the model's thinking process.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Document \n",
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=('post-content', 'post-title','post-header')\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "doc = loader.load()\n",
    "\n",
    "# Split \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "split = text_splitter.split_documents(doc)\n",
    "\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=split, embedding= OpenAIEmbeddings())\n",
    "\n",
    "# Retriever\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#### RETRIEVER & GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "\n",
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain \n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser() \n",
    ") \n",
    "\n",
    "# Question\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e490f578",
   "metadata": {},
   "source": [
    "### Part 2: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e45c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document\n",
    "question = \"What kind of pet do I like?\"\n",
    "document = \"My favorite pet is Penguin.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3316885c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\" Returns the number of tokens in a text string. \"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fbf52ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "query_result = embedding.embed_query(question)\n",
    "document_result = embedding.embed_query(document)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfa6194c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_result: <class 'list'> \n"
     ]
    }
   ],
   "source": [
    "print(f'query_result: {type(query_result)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d7a7e",
   "metadata": {},
   "source": [
    "cosine similarity is recommended for Open AI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3372122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8773972981178011\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1,vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    return dot_product/(norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(similarity)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c52184fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Document\n",
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=('post-content', 'post-title','post-header')\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "doc = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1a021a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Document\n",
    "# Split \n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 1000, chunk_overlap=200)\n",
    "split = text_splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9427025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=split, embedding= OpenAIEmbeddings())\n",
    "\n",
    "# Retriever\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435467e6",
   "metadata": {},
   "source": [
    "### Part 3: Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "302a3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f541e561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5e35bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.')]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4ab0d2",
   "metadata": {},
   "source": [
    "### Part 4: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15043b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based on the following context.\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based on the following context.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "946bdf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be73bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dea52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Task decomposition is a technique used by agents to break down complex tasks into smaller and simpler steps. This allows the agent to plan ahead and tackle the task more effectively.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 248, 'total_tokens': 281, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BmQrTyEcsoYSVSUiMeZiuwIMUzOpD', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--9fd2074f-5d38-4097-9b2b-13625281be2b-0' usage_metadata={'input_tokens': 248, 'output_tokens': 33, 'total_tokens': 281, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "\n",
    "result = chain.invoke({\"context\": docs, \"question\":\"What is Task Decomposition\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6a934b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prateek Kumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub  \n",
    "\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb5a8994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_hub_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa75b03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is the process of breaking down a complex task into smaller and simpler steps to make it more manageable. This can be done through techniques like Chain of Thought (CoT) or Tree of Thoughts, which involve exploring multiple reasoning possibilities at each step. Additionally, task decomposition can also involve using simple prompting, task-specific instructions, or relying on an external classical planner for long-horizon planning.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is task decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c0d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
